{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35cf3e88",
        "outputId": "67d8288c-0684-413f-dee1-ba0bbd676012"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Definisikan Variabel contoh_raw\n",
        "# ==============================================================================\n",
        "contoh_raw = \"\"\"\n",
        "Python is an interpreted high-level general-purpose programming language. Its design\n",
        "philosophy emphasizes code readability with its use of significant indentation.\n",
        "Its language constructs as well as its object-oriented approach aim to\n",
        "help programmers write clear, logical code for small and large-scale projects\n",
        "\"\"\"\n",
        "\n",
        "# 2. Segmentasi Kalimat\n",
        "# ==============================================================================\n",
        "# Use regex to split into sentences (basic approach)\n",
        "sentences = re.split(r'(?<=[.!?])\\s+', contoh_raw.strip().replace('\\n', ' '))\n",
        "print(f\"Jumlah Kalimat: {len(sentences)}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Inisialisasi list untuk menyimpan matriks frekuensi\n",
        "frequency_matrix = []\n",
        "\n",
        "# Define a basic set of English stopwords\n",
        "english_stopwords = set([\n",
        "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
        "    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
        "    'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
        "    'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n",
        "    'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
        "    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
        "    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
        "    'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
        "    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
        "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should',\n",
        "    \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
        "    'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n",
        "    'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
        "    'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
        "])\n",
        "\n",
        "\n",
        "# 3. & 4. Tokenisasi, Pembersihan, dan Perhitungan Frekuensi\n",
        "# ==============================================================================\n",
        "for i, sentence in enumerate(sentences):\n",
        "    # a. Tokenisasi Kata (simple split by whitespace and punctuation)\n",
        "    words = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
        "\n",
        "    # b. Pembersihan: Menghapus stopwords\n",
        "    cleaned_words = [word for word in words if word not in english_stopwords]\n",
        "\n",
        "    # c. Menghitung frekuensi kata dalam kalimat\n",
        "    word_freq = Counter(cleaned_words)\n",
        "    frequency_matrix.append(word_freq)\n",
        "\n",
        "    print(f\"Kalimat {i+1} (Original): {sentence}\")\n",
        "    print(f\"Kalimat {i+1} (Bersih): {cleaned_words}\")\n",
        "    print(f\"Matriks Frekuensi Kalimat {i+1}: {dict(word_freq)}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# 5. Hasil Akhir Matriks Frekuensi Kata-kata\n",
        "# ==============================================================================\n",
        "print(\"\\n## Hasil Matriks Frekuensi Kata per Kalimat (setelah Stopword Removal)\")\n",
        "print(\"Matriks Frekuensi:\")\n",
        "for i, freq in enumerate(frequency_matrix):\n",
        "    print(f\"Kalimat {i+1}: {dict(freq)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah Kalimat: 3\n",
            "--------------------------------------------------\n",
            "Kalimat 1 (Original): Python is an interpreted high-level general-purpose programming language.\n",
            "Kalimat 1 (Bersih): ['python', 'interpreted', 'high', 'level', 'general', 'purpose', 'programming', 'language']\n",
            "Matriks Frekuensi Kalimat 1: {'python': 1, 'interpreted': 1, 'high': 1, 'level': 1, 'general': 1, 'purpose': 1, 'programming': 1, 'language': 1}\n",
            "--------------------------------------------------\n",
            "Kalimat 2 (Original): Its design philosophy emphasizes code readability with its use of significant indentation.\n",
            "Kalimat 2 (Bersih): ['design', 'philosophy', 'emphasizes', 'code', 'readability', 'use', 'significant', 'indentation']\n",
            "Matriks Frekuensi Kalimat 2: {'design': 1, 'philosophy': 1, 'emphasizes': 1, 'code': 1, 'readability': 1, 'use': 1, 'significant': 1, 'indentation': 1}\n",
            "--------------------------------------------------\n",
            "Kalimat 3 (Original): Its language constructs as well as its object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects\n",
            "Kalimat 3 (Bersih): ['language', 'constructs', 'well', 'object', 'oriented', 'approach', 'aim', 'help', 'programmers', 'write', 'clear', 'logical', 'code', 'small', 'large', 'scale', 'projects']\n",
            "Matriks Frekuensi Kalimat 3: {'language': 1, 'constructs': 1, 'well': 1, 'object': 1, 'oriented': 1, 'approach': 1, 'aim': 1, 'help': 1, 'programmers': 1, 'write': 1, 'clear': 1, 'logical': 1, 'code': 1, 'small': 1, 'large': 1, 'scale': 1, 'projects': 1}\n",
            "--------------------------------------------------\n",
            "\n",
            "## Hasil Matriks Frekuensi Kata per Kalimat (setelah Stopword Removal)\n",
            "Matriks Frekuensi:\n",
            "Kalimat 1: {'python': 1, 'interpreted': 1, 'high': 1, 'level': 1, 'general': 1, 'purpose': 1, 'programming': 1, 'language': 1}\n",
            "Kalimat 2: {'design': 1, 'philosophy': 1, 'emphasizes': 1, 'code': 1, 'readability': 1, 'use': 1, 'significant': 1, 'indentation': 1}\n",
            "Kalimat 3: {'language': 1, 'constructs': 1, 'well': 1, 'object': 1, 'oriented': 1, 'approach': 1, 'aim': 1, 'help': 1, 'programmers': 1, 'write': 1, 'clear': 1, 'logical': 1, 'code': 1, 'small': 1, 'large': 1, 'scale': 1, 'projects': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TUGAS"
      ],
      "metadata": {
        "id": "uXvMkOd5FcnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Unduh stopwords Bahasa Indonesia jika belum\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Teks input\n",
        "teks_raw = \"\"\"\n",
        "Dinas Kesehatan Kabupaten/Kota dan Puskesmas juga dapat membuat pos pelayanan\n",
        "vaksinasi COVID-19. Dianjurkan agar setiap sasaran mencari informasi terlebih dahulu terkait\n",
        "jadwal layanan masing-masing fasilitas pelayanan kesehatan atau pos pelayanan vaksinasi\n",
        "\"\"\"\n",
        "\n",
        "# 1. Segmentasi Kalimat (Dokumen)\n",
        "# Setiap kalimat akan dianggap sebagai satu dokumen dalam korpus\n",
        "korpus = [\n",
        "    \"Dinas Kesehatan Kabupaten/Kota dan Puskesmas juga dapat membuat pos pelayanan vaksinasi COVID-19.\",\n",
        "    \"Dianjurkan agar setiap sasaran mencari informasi terlebih dahulu terkait jadwal layanan masing-masing fasilitas pelayanan kesehatan atau pos pelayanan vaksinasi\"\n",
        "]\n",
        "\n",
        "# Dapatkan daftar stopwords Bahasa Indonesia\n",
        "list_stopwords = stopwords.words('indonesian')"
      ],
      "metadata": {
        "id": "HzDqIWjaFevo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi CountVectorizer dengan stopwords Bahasa Indonesia\n",
        "cv = CountVectorizer(stop_words=list_stopwords)\n",
        "\n",
        "# Terapkan CountVectorizer ke korpus (teks)\n",
        "# Fiting dan transform: Membangun kosakata dan menghitung frekuensi\n",
        "word_count_matrix = cv.fit_transform(korpus)\n",
        "\n",
        "# Ambil nama-nama kata (fitur/term)\n",
        "feature_names = cv.get_feature_names_out()\n",
        "\n",
        "# Ubah matriks hasil menjadi DataFrame untuk tampilan yang lebih rapi\n",
        "df_count = pd.DataFrame(word_count_matrix.toarray(),\n",
        "                        index=[f\"Kalimat {i+1}\" for i in range(len(korpus))],\n",
        "                        columns=feature_names)\n",
        "\n",
        "print(\"Matriks Frekuensi Kata (Term Frequency Matrix):\")\n",
        "print(df_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5rMhfKKFlMQ",
        "outputId": "3a353d3a-9a36-4366-a39e-d10a78eaf6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriks Frekuensi Kata (Term Frequency Matrix):\n",
            "           19  covid  dianjurkan  dinas  fasilitas  informasi  jadwal  \\\n",
            "Kalimat 1   1      1           0      1          0          0       0   \n",
            "Kalimat 2   0      0           1      0          1          1       1   \n",
            "\n",
            "           kabupaten  kesehatan  kota  layanan  mencari  pelayanan  pos  \\\n",
            "Kalimat 1          1          1     1        0        0          1    1   \n",
            "Kalimat 2          0          1     0        1        1          2    1   \n",
            "\n",
            "           puskesmas  sasaran  terkait  vaksinasi  \n",
            "Kalimat 1          1        0        0          1  \n",
            "Kalimat 2          0        1        1          1  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi TfidfVectorizer dengan stopwords Bahasa Indonesia\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=list_stopwords)\n",
        "\n",
        "# Terapkan TfidfVectorizer ke korpus\n",
        "# Fiting dan transform: Membangun kosakata dan menghitung bobot TF-IDF\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(korpus)\n",
        "\n",
        "# Ambil nama-nama kata (fitur/term)\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Ubah matriks hasil menjadi DataFrame\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(),\n",
        "                        index=[f\"Kalimat {i+1}\" for i in range(len(korpus))],\n",
        "                        columns=tfidf_feature_names)\n",
        "\n",
        "print(\"\\nMatriks TF-IDF:\")\n",
        "print(df_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbG7C5rSFo9H",
        "outputId": "e2814543-6543-4f4d-b141-22e4ec720bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Matriks TF-IDF:\n",
            "                 19     covid  dianjurkan     dinas  fasilitas  informasi  \\\n",
            "Kalimat 1  0.353003  0.353003    0.000000  0.353003   0.000000   0.000000   \n",
            "Kalimat 2  0.000000  0.000000    0.294325  0.000000   0.294325   0.294325   \n",
            "\n",
            "             jadwal  kabupaten  kesehatan      kota   layanan   mencari  \\\n",
            "Kalimat 1  0.000000   0.353003   0.251164  0.353003  0.000000  0.000000   \n",
            "Kalimat 2  0.294325   0.000000   0.209415  0.000000  0.294325  0.294325   \n",
            "\n",
            "           pelayanan       pos  puskesmas   sasaran   terkait  vaksinasi  \n",
            "Kalimat 1   0.251164  0.251164   0.353003  0.000000  0.000000   0.251164  \n",
            "Kalimat 2   0.418830  0.209415   0.000000  0.294325  0.294325   0.209415  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}